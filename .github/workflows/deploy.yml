name: Deploy Infrastructure and Application

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy (dev or prod)'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod

env:
  AWS_REGION: us-west-2
  TERRAFORM_VERSION: 1.5.7
  PROJECT_PREFIX: pet-hospital

permissions:
  id-token: write
  contents: write

jobs:
  setup_state_storage:
    name: Setup Terraform State Storage
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set environment
        id: set-env
        run: |
          if [ "${{ github.event.inputs.environment }}" != "" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            echo "environment=dev" >> $GITHUB_OUTPUT
          fi

      - name: Setup Terraform State Storage
        run: |
          # Check if S3 bucket already exists
          if aws s3api head-bucket --bucket ${{ env.PROJECT_PREFIX }}-terraform-state-${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Terraform state storage already exists. Skipping creation."
          else
            echo "Creating Terraform state storage..."
            ./infrastructure/setup-state-storage.sh --prefix ${{ env.PROJECT_PREFIX }} --region ${{ env.AWS_REGION }}
          fi

  terraform:
    name: Terraform
    needs: setup_state_storage
    runs-on: ubuntu-latest
    outputs:
      cluster_name: ${{ steps.terraform-outputs.outputs.cluster_name }}
      application_url: ${{ steps.terraform-outputs.outputs.application_url }}
      aws_account_id: ${{ steps.get-aws-account.outputs.aws_account_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Terraform Init
        working-directory: ./infrastructure
        run: terraform init

      - name: Terraform Plan
        working-directory: ./infrastructure
        run: |
          if [ "${{ needs.setup_state_storage.outputs.environment }}" == "prod" ]; then
            terraform plan -var-file=environments/prod.tfvars -out=tfplan
          else
            terraform plan -var-file=environments/terraform.tfvars -out=tfplan
          fi

      - name: Terraform Apply
        working-directory: ./infrastructure
        run: terraform apply -auto-approve tfplan

      - name: Get Terraform Outputs
        working-directory: ./infrastructure
        run: |
          # Get outputs with all the debug info
          FULL_CLUSTER_NAME=$(terraform output -raw cluster_name)
          FULL_APPLICATION_URL=$(terraform output -raw application_url)
          
          echo "Full Cluster name output: '$FULL_CLUSTER_NAME'"
          echo "Full Application URL output: '$FULL_APPLICATION_URL'"
          
          # Extract just the stdout part using grep and sed
          CLUSTER_NAME=$(echo "$FULL_CLUSTER_NAME" | grep -o "stdout: .*" | sed 's/stdout: //')
          APPLICATION_URL=$(echo "$FULL_APPLICATION_URL" | grep -o "stdout: .*" | sed 's/stdout: //')
          
          # If grep didn't find the pattern, try a simpler approach
          if [ -z "$CLUSTER_NAME" ]; then
            # Just take the first line and remove any non-printable characters
            CLUSTER_NAME=$(echo "$FULL_CLUSTER_NAME" | head -n 1 | tr -cd '[:print:]')
          fi
          
          if [ -z "$APPLICATION_URL" ]; then
            # Just take the first line and remove any non-printable characters
            APPLICATION_URL=$(echo "$FULL_APPLICATION_URL" | head -n 1 | tr -cd '[:print:]')
          fi
          
          echo "Extracted Cluster name: '$CLUSTER_NAME'"
          echo "Extracted Application URL: '$APPLICATION_URL'"
          
          # Write to files and then to GITHUB_ENV to avoid any issues
          echo "CLUSTER_NAME=${CLUSTER_NAME}" > cluster_env.txt
          echo "APPLICATION_URL=${APPLICATION_URL}" > app_url_env.txt
          
          cat cluster_env.txt >> $GITHUB_ENV
          cat app_url_env.txt >> $GITHUB_ENV

      - name: Get AWS Account ID
        id: get-aws-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS Account ID: $AWS_ACCOUNT_ID"
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT

      - name: Set Job Outputs
        id: terraform-outputs
        run: |
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "application_url=$APPLICATION_URL" >> $GITHUB_OUTPUT

  build_and_push:
    name: Build and Push Docker Images
    needs: [terraform]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [pet-service, hospital-service, doctor-service, billing-service, insurance-service, visit-service, vet-service, frontend]
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Construct ECR URL
        id: construct-ecr-url
        run: |
          SERVICE="${{ matrix.service }}"
          AWS_ACCOUNT_ID="${{ needs.terraform.outputs.aws_account_id }}"
          
          echo "Service: $SERVICE"
          echo "AWS Account ID from output: $AWS_ACCOUNT_ID"
          
          # If AWS_ACCOUNT_ID is empty, get it directly
          if [ -z "$AWS_ACCOUNT_ID" ]; then
            echo "AWS Account ID is empty, getting it directly"
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "AWS Account ID from direct call: $AWS_ACCOUNT_ID"
          fi
          
          # Ensure AWS_ACCOUNT_ID is not empty
          if [ -z "$AWS_ACCOUNT_ID" ]; then
            echo "Error: AWS Account ID is still empty"
            exit 1
          fi
          
          ECR_URL="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.PROJECT_PREFIX }}-${SERVICE}"
          
          echo "ECR URL: $ECR_URL"
          echo "ecr_url=$ECR_URL" >> $GITHUB_OUTPUT

      - name: Check if service directory exists
        id: check-dir
        run: |
          SERVICE="${{ matrix.service }}"
          DIR_PATH="${{ matrix.service == 'frontend' && './frontend' || format('./backend/{0}', matrix.service) }}"
          
          if [ -d "$DIR_PATH" ]; then
            echo "Directory $DIR_PATH exists"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "Directory $DIR_PATH does not exist, skipping"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      # Fix frontend nginx.conf if this is the frontend service
      - name: Fix frontend nginx.conf
        if: matrix.service == 'frontend' && steps.check-dir.outputs.exists == 'true'
        run: |
          echo "Fixing frontend nginx.conf to handle missing services..."
          
          # Create a completely new nginx.conf file with all the fixes
          cat > ./frontend/nginx.conf << 'EOF'
          server {
              listen 80;
              server_name localhost;
              root /usr/share/nginx/html;
              index index.html;

              location / {
                  try_files $uri $uri/ /index.html;
              }

              location /api/pets {
                  proxy_pass http://pet-service:3000/pets;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              location /api/hospitals {
                  proxy_pass http://hospital-service:3000/hospitals;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              location /api/doctors {
                  proxy_pass http://doctor-service:3000/doctors;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              # Billing service not yet implemented
              location /api/billing {
                  return 501 '{"error": "Billing service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Insurance service not yet implemented
              location /api/insurance {
                  return 501 '{"error": "Insurance service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Visit service not yet implemented
              location /api/visits {
                  return 501 '{"error": "Visit service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Vet service not yet implemented
              location /api/vets {
                  return 501 '{"error": "Vet service not yet implemented"}';
                  add_header Content-Type application/json;
              }
          }
          EOF
          
          echo "Created new nginx.conf:"
          cat ./frontend/nginx.conf

      - name: Build and push Docker image
        if: steps.check-dir.outputs.exists == 'true'
        uses: docker/build-push-action@v4
        with:
          context: ${{ matrix.service == 'frontend' && './frontend' || format('./backend/{0}', matrix.service) }}
          push: true
          tags: ${{ steps.construct-ecr-url.outputs.ecr_url }}:latest,${{ steps.construct-ecr-url.outputs.ecr_url }}:${{ github.sha }}
          build-args: |
            AWS_REGION=${{ env.AWS_REGION }}

  deploy_to_eks:
    name: Deploy to EKS
    needs: [setup_state_storage, terraform, build_and_push]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ needs.terraform.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Install ArgoCD CLI
        run: |
          curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
          sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
          rm argocd-linux-amd64

      - name: Wait for ArgoCD to be ready
        run: |
          echo "Waiting for ArgoCD server to be ready..."
          kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd

      - name: Update AWS Load Balancer Controller IAM Policy
        run: |
          # Update the IAM policy for the AWS Load Balancer Controller
          echo "Updating AWS Load Balancer Controller IAM Policy..."
          
          # Create the policy document
          cat > alb-policy.json << 'EOF'
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": [
                  "elasticloadbalancing:DescribeLoadBalancers",
                  "elasticloadbalancing:DescribeTargetGroups",
                  "elasticloadbalancing:DescribeTargetHealth",
                  "elasticloadbalancing:DescribeListeners",
                  "elasticloadbalancing:DescribeRules",
                  "elasticloadbalancing:DescribeListenerCertificates",
                  "elasticloadbalancing:DescribeTags",
                  "elasticloadbalancing:DescribeLoadBalancerAttributes",
                  "elasticloadbalancing:DescribeTargetGroupAttributes",
                  "elasticloadbalancing:DescribeListenerAttributes",
                  "elasticloadbalancing:ModifyLoadBalancerAttributes",
                  "elasticloadbalancing:ModifyTargetGroupAttributes",
                  "elasticloadbalancing:ModifyListener",
                  "elasticloadbalancing:ModifyRule",
                  "elasticloadbalancing:SetSubnets",
                  "elasticloadbalancing:SetSecurityGroups",
                  "elasticloadbalancing:SetIpAddressType",
                  "elasticloadbalancing:CreateLoadBalancer",
                  "elasticloadbalancing:CreateTargetGroup",
                  "elasticloadbalancing:CreateListener",
                  "elasticloadbalancing:CreateRule",
                  "elasticloadbalancing:DeleteLoadBalancer",
                  "elasticloadbalancing:DeleteTargetGroup",
                  "elasticloadbalancing:DeleteListener",
                  "elasticloadbalancing:DeleteRule",
                  "elasticloadbalancing:RegisterTargets",
                  "elasticloadbalancing:DeregisterTargets",
                  "elasticloadbalancing:AddTags",
                  "elasticloadbalancing:RemoveTags",
                  "elasticloadbalancing:AddListenerCertificates",
                  "elasticloadbalancing:RemoveListenerCertificates",
                  "elasticloadbalancing:SetWebAcl",
                  "ec2:DescribeVpcs",
                  "ec2:DescribeSubnets",
                  "ec2:DescribeSecurityGroups",
                  "ec2:DescribeInstances",
                  "ec2:DescribeInternetGateways",
                  "ec2:DescribeNatGateways",
                  "ec2:DescribeRouteTables",
                  "ec2:DescribeNetworkInterfaces",
                  "ec2:DescribeAvailabilityZones",
                  "ec2:CreateSecurityGroup",
                  "ec2:CreateTags",
                  "ec2:DeleteSecurityGroup",
                  "ec2:AuthorizeSecurityGroupIngress",
                  "ec2:RevokeSecurityGroupIngress",
                  "ec2:DeleteTags",
                  "acm:ListCertificates",
                  "acm:DescribeCertificate",
                  "iam:CreateServiceLinkedRole",
                  "wafv2:GetWebACL",
                  "wafv2:GetWebACLForResource",
                  "wafv2:AssociateWebACL",
                  "wafv2:DisassociateWebACL",
                  "shield:GetSubscriptionState",
                  "shield:DescribeProtection",
                  "shield:CreateProtection",
                  "shield:DeleteProtection",
                  "cognito-idp:DescribeUserPoolClient"
                ],
                "Resource": "*"
              }
            ]
          }
          EOF
          
          # Get the policy ARN
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='pet-hospital-eks-cluster-AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)
          
          if [ -n "$POLICY_ARN" ]; then
            echo "IAM policy exists, checking if update is needed..."
            
            # Get the default version ID
            DEFAULT_VERSION=$(aws iam get-policy --policy-arn $POLICY_ARN --query 'Policy.DefaultVersionId' --output text)
            
            # Get the current policy document
            CURRENT_POLICY=$(aws iam get-policy-version --policy-arn $POLICY_ARN --version-id $DEFAULT_VERSION --query 'PolicyVersion.Document' --output json)
            
            # Compare with our policy document (normalize both by removing whitespace)
            CURRENT_NORMALIZED=$(echo "$CURRENT_POLICY" | jq -c .)
            NEW_NORMALIZED=$(cat alb-policy.json | jq -c .)
            
            if [ "$CURRENT_NORMALIZED" != "$NEW_NORMALIZED" ]; then
              echo "Policy needs to be updated..."
              
              # List all non-default policy versions
              echo "Listing policy versions..."
              aws iam list-policy-versions --policy-arn $POLICY_ARN
              
              # Get the number of versions
              VERSION_COUNT=$(aws iam list-policy-versions --policy-arn $POLICY_ARN --query 'length(Versions)' --output text)
              echo "Current version count: $VERSION_COUNT"
              
              # If we have 5 versions, we need to delete one
              if [ "$VERSION_COUNT" -ge 5 ]; then
                # Get the oldest non-default version ID
                OLDEST_VERSION=$(aws iam list-policy-versions --policy-arn $POLICY_ARN --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text | tr '\t' '\n' | head -1)
                echo "Deleting oldest policy version: $OLDEST_VERSION"
                aws iam delete-policy-version --policy-arn $POLICY_ARN --version-id $OLDEST_VERSION
              fi
              
              # Create new policy version
              echo "Creating new policy version..."
              aws iam create-policy-version --policy-arn $POLICY_ARN --policy-document file://alb-policy.json --set-as-default
              echo "IAM policy updated successfully"
            else
              echo "IAM policy is already up to date, no changes needed"
            fi
          else
            echo "IAM policy not found, creating new policy..."
            aws iam create-policy --policy-name pet-hospital-eks-cluster-AWSLoadBalancerControllerIAMPolicy --policy-document file://alb-policy.json
            echo "IAM policy created successfully"
          fi
          
      - name: Increase IP allocation for AWS CNI
        run: |
          # Increase the IP address allocation for the AWS CNI plugin
          echo "Increasing IP address allocation for AWS CNI..."
          kubectl set env daemonset aws-node -n kube-system WARM_IP_TARGET=5
          kubectl set env daemonset aws-node -n kube-system MINIMUM_IP_TARGET=10
          
          # Wait for the changes to take effect
          echo "Waiting for AWS CNI changes to take effect..."
          sleep 30

      - name: Create namespace and ECR secret
        run: |
          # Create namespace
          echo "Creating namespace: pethospital-dev"
          kubectl create namespace pethospital-dev --dry-run=client -o yaml | kubectl apply -f -
          kubectl label namespace pethospital-dev --overwrite argocd.argoproj.io/managed-by=argocd
          
          # Create ECR registry secret - fixed format
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_TOKEN=$(aws ecr get-login-password --region ${AWS_REGION})
          
          # Create a proper Docker config JSON with explicit base64 encoding
          DOCKER_CONFIG="{\"auths\":{\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com\":{\"auth\":\"$(echo -n "AWS:${ECR_TOKEN}" | base64 -w 0)\"}}}"
          
          # Create the secret directly without base64 encoding the entire JSON
          kubectl create secret docker-registry ecr-registry-secret \
            --namespace=pethospital-dev \
            --docker-server=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com \
            --docker-username=AWS \
            --docker-password=${ECR_TOKEN} \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Verify the secret was created correctly
          echo "Verifying ECR registry secret..."
          kubectl get secret ecr-registry-secret -n pethospital-dev
          
          # Create a ConfigMap to store the ECR repository URL for use in deployments
          echo "Creating ConfigMap with ECR repository URL..."
          kubectl create configmap ecr-config \
            --namespace=pethospital-dev \
            --from-literal=repository_url=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Fix Kubernetes manifests
        run: |
          echo "Updating Kubernetes manifests with new image tags..."
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE_URL="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          
          # Use commit SHA for image tags
          IMAGE_TAG="${GITHUB_SHA}"
          echo "Using image tag: ${IMAGE_TAG}"
          
          # Update frontend.yaml to use the correct ECR URL and image tag
          sed -i "s|\${ECR_REPOSITORY_URL}/frontend:latest|${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:${IMAGE_TAG}|g" ./k8s/base/frontend.yaml
          sed -i "s|image: .*dkr.ecr.*.amazonaws.com/pet-hospital-frontend:.*|image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:${IMAGE_TAG}|g" ./k8s/base/frontend.yaml
          
          # Check if other service manifests exist and update them
          for service in pet-service hospital-service doctor-service; do
            if [ -f "./k8s/base/${service}.yaml" ]; then
              echo "Updating ${service}.yaml..."
              sed -i "s|\${ECR_REPOSITORY_URL}/${service}:latest|${ECR_BASE_URL}/${PROJECT_PREFIX}-${service}:${IMAGE_TAG}|g" ./k8s/base/${service}.yaml
              sed -i "s|image: .*dkr.ecr.*.amazonaws.com/pet-hospital-${service}:.*|image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-${service}:${IMAGE_TAG}|g" ./k8s/base/${service}.yaml
            fi
          done
          
          # Update deployment patches to include explicit image references with tags
          echo "Updating deployment patches..."
          cat > ./k8s/overlays/dev/patches/deployment-patches.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: pet-service
          spec:
            template:
              spec:
                containers:
                  - name: pet-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-pet-service:${IMAGE_TAG}
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: hospital-service
          spec:
            template:
              spec:
                containers:
                  - name: hospital-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-hospital-service:${IMAGE_TAG}
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: doctor-service
          spec:
            template:
              spec:
                containers:
                  - name: doctor-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-doctor-service:${IMAGE_TAG}
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: frontend
          spec:
            template:
              spec:
                containers:
                  - name: frontend
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: API_URL
                        value: "http://frontend-ingress"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:${IMAGE_TAG}
          EOF
          
          echo "Updated deployment patches:"
          cat ./k8s/overlays/dev/patches/deployment-patches.yaml
          
          # Update ECR secret with proper format
          echo "Updating ECR secret..."
          cat > ./k8s/overlays/dev/ecr-secret.yaml << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: ecr-registry-secret
            namespace: pethospital-dev
          type: kubernetes.io/dockerconfigjson
          data:
            # This is a placeholder that will be replaced by the actual credentials in the GitHub workflow
            .dockerconfigjson: eyJhdXRocyI6eyIke0FXU19BQ0NPVU5UX0lEfS5ka3IuZWNyLiR7QVdTX1JFR0lPTn0uYW1hem9uYXdzLmNvbSI6eyJhdXRoIjoiUVZkVE9rVkRVaUJRUVZOVFYwOVNSQT09In19fQ==
          EOF
          
          echo "Updated ECR secret:"
          cat ./k8s/overlays/dev/ecr-secret.yaml

      - name: Commit and push updated manifests
        run: |
          echo "Committing and pushing updated manifests with new image tags..."

          # Configure Git
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"

          # Only stage the k8s directory changes
          git add ./k8s/

          # Check if there are changes to commit
          if ! git diff --staged --quiet; then
            # Commit changes
            git commit -m "Update image tags to ${GITHUB_SHA} [skip ci]"

            # Pull with rebase to integrate any remote changes
            git pull --rebase origin main || git reset --hard origin/main

            # Push the changes
            git push origin main
          else
            echo "No changes to commit"
          fi

      - name: Verify ArgoCD Application
        run: |
          echo "Verifying ArgoCD application status..."

          # Check if ArgoCD application exists
          if kubectl get application pethospital -n argocd &>/dev/null; then
            echo "ArgoCD application exists, checking status..."
            kubectl get application pethospital -n argocd -o yaml
          else
            echo "Creating ArgoCD application..."

            # Create a YAML file for the application
            echo "apiVersion: argoproj.io/v1alpha1" > argocd-app.yaml
            echo "kind: Application" >> argocd-app.yaml
            echo "metadata:" >> argocd-app.yaml
            echo "  name: pethospital" >> argocd-app.yaml
            echo "  namespace: argocd" >> argocd-app.yaml
            echo "spec:" >> argocd-app.yaml
            echo "  project: default" >> argocd-app.yaml
            echo "  source:" >> argocd-app.yaml
            echo "    repoURL: https://github.com/${{ github.repository }}.git" >> argocd-app.yaml
            echo "    targetRevision: main" >> argocd-app.yaml
            echo "    path: k8s/overlays/${{ needs.setup_state_storage.outputs.environment }}" >> argocd-app.yaml
            echo "  destination:" >> argocd-app.yaml
            echo "    server: https://kubernetes.default.svc" >> argocd-app.yaml
            echo "    namespace: pethospital-dev" >> argocd-app.yaml
            echo "  syncPolicy:" >> argocd-app.yaml
            echo "    automated:" >> argocd-app.yaml
            echo "      prune: true" >> argocd-app.yaml
            echo "      selfHeal: true" >> argocd-app.yaml
            echo "      allowEmpty: false" >> argocd-app.yaml
            echo "    syncOptions:" >> argocd-app.yaml
            echo "    - CreateNamespace=true" >> argocd-app.yaml
            echo "    - Validate=true" >> argocd-app.yaml
            echo "    - PrunePropagationPolicy=foreground" >> argocd-app.yaml
            echo "    - PruneLast=true" >> argocd-app.yaml

            # Apply the manifest
            kubectl apply -f argocd-app.yaml
          fi

          echo "ArgoCD will automatically sync the application with the updated manifests"

      - name: Debug Pod Status
        run: |
          echo "Checking pod status in pethospital-dev namespace..."
          kubectl get pods -n pethospital-dev
          
          echo "Checking pod details..."
          for pod in $(kubectl get pods -n pethospital-dev -o jsonpath='{.items[*].metadata.name}'); do
            echo "==== Details for pod: $pod ===="
            kubectl describe pod $pod -n pethospital-dev
          done
          
          echo "Checking events..."
          kubectl get events -n pethospital-dev --sort-by='.lastTimestamp'
          
          echo "Checking ingress status..."
          kubectl get ingress -n pethospital-dev -o wide
          
          echo "Checking services..."
          kubectl get services -n pethospital-dev -o wide

      - name: Output Application URL
        run: |
          echo "Application URL: ${{ needs.terraform.outputs.application_url }}"
          echo "APPLICATION_URL=${{ needs.terraform.outputs.application_url }}" >> $GITHUB_ENV

  notify:
    name: Notify Deployment Status
    needs: [deploy_to_eks, terraform]
    runs-on: ubuntu-latest
    steps:
      - name: Deployment Success
        run: |
          echo "Deployment completed successfully!"
          echo "Application URL: ${{ needs.terraform.outputs.application_url }}"
